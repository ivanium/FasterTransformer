Inside docker
/FasterTransformer/build /home/gedatsu/FasterTransformer/scripts
Running mnfd_llama with /home/gedatsu/FasterTransformer/tmp/original_4gpu_v100/manifold_llama-1-1.ini
[FT][WARNING] Skip NCCL initialization since requested tensor/pipeline parallel sizes are equals to 1.
Total ranks: 1.
Device Tesla V100-SXM2-16GB
P0-0 is running with GPU #0.
Device Tesla V100-SXM2-16GB
P0-1 is running with GPU #1.
Device Tesla V100-SXM2-16GB
P0-2 is running with GPU #2.
Device Tesla V100-SXM2-16GB
P0-3 is running with GPU #3.
[WARNING] gemm_config.in is not found; using default GEMM algo
[WARNING] gemm_config.in is not found; using default GEMM algo
[WARNING] gemm_config.in is not found; using default GEMM algo
[WARNING] gemm_config.in is not found; using default GEMM algo
after allocation    : free: 11.92 GB, total: 15.77 GB, used:  3.85 GB
after allocation    : free: 11.92 GB, total: 15.77 GB, used:  3.85 GB
after allocation    : free: 11.92 GB, total: 15.77 GB, used:  3.85 GB
after allocation    : free: 11.92 GB, total: 15.77 GB, used:  3.85 GB
Writing 7 elements
    1   450  7483   310  7551   338  1522 

zeroCount = 0
[INFO] request_batch_size 1 beam_width 1 head_num 32 size_per_head 128 total_output_len 7 decoder_layers 32 vocab_size 32000 FT-CPP-decoding-beamsearch-time 32.44 ms
[INFO] request_batch_size 1 beam_width 1 head_num 32 size_per_head 128 total_output_len 7 decoder_layers 32 vocab_size 32000 FT-CPP-decoding-beamsearch-time 32.44 ms
[INFO] request_batch_size 1 beam_width 1 head_num 32 size_per_head 128 total_output_len 7 decoder_layers 32 vocab_size 32000 FT-CPP-decoding-beamsearch-time 32.43 ms
[INFO] request_batch_size 1 beam_width 1 head_num 32 size_per_head 128 total_output_len 7 decoder_layers 32 vocab_size 32000 FT-CPP-decoding-beamsearch-time 32.43 ms
/home/gedatsu/FasterTransformer/scripts
"<s> The capital of China is Be"
